{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from util import get_wikipedia_data\n",
    "from brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx\n",
    "\n",
    "from markov import get_bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 2000, 661, 2000, 1639, 74, 1850, 47, 2000, 16, 2000, 561, 1129, 1408, 1202, 26, 71, 483, 27, 21, 97, 2000, 224, 182, 15]\n",
      "[('START', 0), ('END', 1), ('man', 2), ('paris', 3), ('britain', 4), ('england', 5), ('king', 6), ('woman', 7), ('rome', 8), ('london', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print([(k, v) for k, v in word2idx.items()][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2001\n"
     ]
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "print(\"Vocab size:\", V)\n",
    "\n",
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a matrix where:\n",
    "# row = last word\n",
    "# col = current word\n",
    "# value at [row, col] = p(current word | last word)\n",
    "bigram_probs = get_bigram_probs(sentences,\n",
    "                                V,\n",
    "                                start_idx,\n",
    "                                end_idx,\n",
    "                                smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 2001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    a = a - a.max()\n",
    "    exp_a = np.exp(a)\n",
    "    return exp_a / exp_a.sum(axis=1, keepdims=True)\n",
    "\n",
    "# what is the loss if we set W = log(bigram_probs)?\n",
    "W_bigram = np.log(bigram_probs)\n",
    "bigram_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = datetime.now()\n",
    "for epoch in range(epochs):\n",
    "    # shuffle sentences at each epoch\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    j = 0  # keep track of iterations\n",
    "    for sentence in sentences:\n",
    "        # convert sentence into one-hot encoded inputs and targets\n",
    "        sentence = [start_idx] + sentence + [end_idx]\n",
    "        n = len(sentence)\n",
    "        inputs = np.zeros((n - 1, V))\n",
    "        targets = np.zeros((n - 1, V))\n",
    "        inputs[np.arange(n - 1), sentence[:n - 1]] = 1\n",
    "        targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "\n",
    "        # get output predictions\n",
    "        predictions = softmax(inputs.dot(W))\n",
    "\n",
    "        # do a gradient descent step\n",
    "        W = W - lr * inputs.T.dot(predictions - targets)\n",
    "\n",
    "        # keep track of the loss\n",
    "        loss = -np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # keep track of the bigram loss\n",
    "        # only do it for the first epoch to avoid redundancy\n",
    "        if epoch == 0:\n",
    "            bigram_predictions = softmax(inputs.dot(W_bigram))\n",
    "            bigram_loss = -np.sum(targets * np.log(bigram_predictions)) / (n - 1)\n",
    "            bigram_losses.append(bigram_loss)\n",
    "\n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)),\n",
    "                  \"loss:\", loss)\n",
    "        j += 1\n",
    "\n",
    "print(\"Elapsed time training:\", datetime.now() - t0)\n",
    "plt.plot(losses)\n",
    "\n",
    "# plot a horizontal line for the bigram loss\n",
    "avg_bigram_loss = np.mean(bigram_losses)\n",
    "print(\"avg_bigram_loss:\", avg_bigram_loss)\n",
    "plt.axhline(y=avg_bigram_loss, color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1cf8fdfa5ff0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# convert sentence into one-hot encoded inputs and targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     sentence = [start_idx] + sentence + [end_idx]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "snts = [[1, 3, 5], [2, 5, 6, 1, 3], [2, 4]]\n",
    "\n",
    "V = 10\n",
    "\n",
    "for s in snts[0]:\n",
    "    # convert sentence into one-hot encoded inputs and targets\n",
    "#     sentence = [start_idx] + sentence + [end_idx]\n",
    "    n = len(s)\n",
    "    inputs = np.zeros((n - 1, V))\n",
    "    targets = np.zeros((n - 1, V))\n",
    "    print(inputs)\n",
    "    inputs[np.arange(n - 1), s[:n - 1]] = 1\n",
    "    targets[np.arange(n - 1), s[1:]] = 1\n",
    "    print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snts[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
